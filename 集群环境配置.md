# 集群环境配置
## 一、附录
　　本文档适用于操作系统版本ubuntu14.04;  
所安装软件版本为：JDK 1.7及以上版本；hadoop-2.6.3版;  
hive-1.2.1；scala-2.10.4；spark-1.6.1；mysql 5.5.41版本  
　　由于在/etc/profie.d/cluster.sh中配置了环境变量，服务器重启后才能生效，  
下面配置过程中可能引用到环境变量，若环境变量没有生效，可执行命令：source /etc/profile  
也可以可直接使用变量内容，现总结如下：

```   
JAVE_HOME=/opt/modules/jdk
HADOOP_HOME=/opt/modules/hadoop
SCALA_HOME=/opt/modules/scala
SPARK_HOME=/opt/modules/spark
HIVE_HOME=/opt/modules/hive
```
##  二、配置主机信息（slave\*和master\*都配置）
1. 	创建hadoop用户  
    方法：  
    ①	sudo adduser hadoop，新建用户密码为eflagrd，其他选项回车默认；  
    ```
    sudo adduser hadoop
    ```
![](test/1.jpg)  

   ②	赋予etc/sudoers可写权限，sudo chmod u+w /etc/sudoers
   ```
   sudo chmod u+w /etc/sudoers
   ```
![](test/2.jpg)

  ③	分配给hadoop用户管理员权限，sudo vim /etc/sudoers  
 ```
 sudo vim /etc/sudoers
 ```
 在第23行和第24行新增两行，添加下图内容
 ```
 hadoop  All=(ALL)ALL  
 hadoop  ALL=NOPASSWD:/opt/omatrix/tool/dumper.sh,/opt/omatrix/tool/close_exit_process.sh,/opt/omatrix/tool/cset.sh
 ```  
  ④	执行命令sudo chmod u-w /etc/sudoers。禁用sudoers文件的写权限  
  ```
  sudo chmod u-w /etc/sudoers
  ```
![](test/3.jpg)  

  **⑤	分配了hadoop管理员权限，以下操作均在hadoop用户上操作。**  
*&nbsp;&nbsp;说明； 以上②－④步主要分配hadoop用户管理权限。也可以用以下操作
    使用命令sudo visudo –f /etc/sudoers 进行编辑sudoers文件。*  
    ```
     sudo visudo -f /etc/sudoers
    ```  
    进入编辑界面，在23行，24行添加如下内容。  
	```
    hadoop  All=(ALL)ALL  
    hadoop  ALL=NOPASSWD:/opt/omatrix/tool/dumper.sh,/opt/omatrix/tool/close_exit_process.sh,/opt/omatrix/tool/cset.sh
   ```  
2. 配置主机名称  
　　作为master节点的主机，配置hostname名称为master\* （\*=1、2、3…）；作为slave节点的主机，配置hostname为slave\*。  
  ①	sudo vim /etc/hostname，将原有名称ubuntu修改为master*或者slave*；
  ```
  sudo vim /etc/hostname
  ```
![](test/4.jpg)  
  ②	sudo vim etc/hosts ，将原第2行删除，增加slave*IP	 主机名称，master*IP 主机名称  
  ```
  sudo vim /etc/hosts
  ```  
  ![](test/5.jpg)  

3. 配置主机IP  
　　将master*和slave*配置成同网段IP。  
   ```
    sudo vim /etc/network/interfaces
   ```  
  ![](test/6.jpg)  

4. 配置主机间ssh免密码登陆  
    ①	ssh-keygen –t dsa；按enter键执行到最后
    ```
    ssh-keygen –t dsa
    ```  
    ![](test/7.jpg)  

    ②	ssh-copy-id hadoop@IPaddress。
    ```
    ssh-copy-id hadoop@IPaddress  
    ```  
    ![](test/9.jpg)  

    *&nbsp;&nbsp;需要完成本地无密码登陆以及master*和slave*间无密码登陆。*  
5. 部署各个软件安装环境
    ①	在/opt下新建softwares文件夹，用于存放各软件安装包。
    ```
    sudo mkdir /opt/softwares  
    ```  
    ②	将softwares用户改成hadoop  
    ```
    sudo chown –R hadoop:hadoop /opt/softwares  
    ```  
    ③	在/opt下新建packages文件夹，用于存放各软件。  
    ```
    sudo mkdir /opt/packages  
    ```  
    ④	将packages用户改成hadoop  
    ```  
    sudo chown –R hadoop:hadoop /opt/packages  
    ```  
    ⑤	在/opt下新建modules文件夹，用于存放各软件链接  
    ```  
    sudo mkdir /opt/modules  
    ```
    ⑥	将modules用户改成hadoop  
    ```
    sudo chown –R hadoop:hadoop /opt/modules  
    ```  
6. 增加环境变量脚本  
　　在/etc/profile.d下增加脚本cluster.sh，用于开机启动环境变量配置；  
	```
   	sudo touch /etc/protile.d/cluster.sh  
   	sudo chmod +x /etc/profile.d/cluster.sh  
	```  
7. 创建软链接至硬盘  
　　在/mnt/raid5(master\*)或者/mnt/raid0(slave\*)下创建data文件夹，  
在/opt下创建data软链接至/mnt/raid5或者/mnt/raid0下的data文件夹。    
下例为配置master*，若配置slave*，将下例中raid0改为raid5即可。  
	```
	sudo mkdir /mnt/raid5/data
	sudo chown –R /mnt/raid5/data
	sudo ln –s /mnt/raid5/data  /opt/data
	```  
![](test/8.jpg)  

## 三、安装JDK（slave\*和master\*都配置）
1. 下载JDK  
　　下载jdk1.8.0_74.tar.gz至/opt/softwares。  
2. 解压JDK至指定目录  
    解压jdk1.8.0_74.tar.gz至/opt/packages。  
    ```  
    tar zxvf  /opt/sofawares/jdk1.8.0_74.tar.gz –C /opt/packages  
    ```  
3. 创建JDK软链接  
    在/opt/modules下创建jdk软链接至/opt/packages/jdk1.8.0_74。  
    ```  
    ln –s /opt/packages/jdk1.8.0_74/  /opt/modules/jdk  
   ```  
4. 配置jdk环境变量。  
    ```
    sudo vim /etc/profile.d/cluster.sh  
    ```
添加内容  
    ```  
    export JAVA_HOME=/opt/modules/jdk  
    export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar  

    export PATH=$PATH:$JAVA_HOME/bin  
    ```  
    ![](test/10.jpg)  

##  四、安装Hadoop（slave\*和master\*都配置）  

1.	下载hadoop  
　　下载hadoop-2.6.3.tar.gz至/opt/sofawares。　　
2.	解压hadoop至指定目录  
　　解压hadoop-2.6.3.tar.gz至/opt/packages。  
    ```
    tar zxvf  /opt/sofawares/hadoop-2.6.3.tar.gz  –C  /opt/packages  
    ```
3.	创建hadoop软链接  
　　在/opt/modules下创建hadoop软链接至/opt/packages/hadoop-2.6.3。  
    ```
    ln –s /opt/packages/hadoop-2.6.3/  /opt/modules/hadoop  
    ```
4.	配置hadoop环境变量  
    ```
    sudo vim /etc/profile.d/cluster.sh  
    ```
　　在6、7行添加如下内容  
    ```
    export HADOOP_HOME=/opt/modules/hadoop  
    export PATH=$PATH:$HADOOP_HOME/bin  
    ```
    ![](test/11.jpg)  
5.	创建用于配置文件的文件夹  
　　在/mnt/raid0/data（slave*）或者/mnt/raid5/data(master*)下创建hadoop文件夹，  
然后在该hadoop文件夹下面创建dataNode、nameNode、tmp三个文件夹。  
    ```
  	mkdir /mnt/raid0/data/hadoop  
    mkdir /mnt/raid0/data/hadoop/dataNode  
    mkdir /mnt/raid0/data/hadoop/nameNode    
  	mkdir /mnt/raid0/data/hadoop/tmp
    ```  
    ![](test/12.jpg)  

6. 修改配置文件hadoop-env.sh  
　　修改配置文件hadoop-env.sh；第25行赋值JAVA_HOME为本机jdk实际所在目录，  
我们之前在/opt/modules建立了软连接，现在赋值JAVA_HOME为软链接位置即可，  
所以修改hadoop-env.sh内的JAVA_HOME=/opt/modules/jdk。  
    ```
    vim  /opt/packages/hadoop-2.6.3/etc/hadoop/hadoop-env.sh  
    ```  
    ![](test/13.jpg)  

7. 修改配置文件core-site.xml  
　　在目录/opt/packages/hadoop-2.6.3/etc/hadoop/下  
修改配置文件core-site.xml，添加配置，添加内容为：  
    ```
    <configuration>
<property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/data/hadoop/tmp</value>
</property>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master1:9000</value>
</property>
<property>
    <name>io.file.buffer.size</name>
    <value>4096</value>
</property>
</configuration>
    ```  
    添加前：    
    ![](test/14.jpg)  

    添加后：  
    ![](test/15.jpg)  
8. 修改配置文件hdfs-site.xml  
　　在目录 /opt/packages/hadoop-2.6.3/etc/hadoop/下
修改配置文件hdfs-site.xml，添加配置，添加内容为：  
   ①	slave*修改内容：  
   ```
   <configuration>
<property>
    <name>dfs.namenode.http-address</name>
    <value>master1:50070</value>
</property>
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>master1:50090</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/opt/data/hadoop/nameNode</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///opt/data/hadoop/dataNode</value>
</property>
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.webhdfs.enable</name>
    <value>8192</value>
</property>
</configuration>
  ```  
  添加前：  
  ![](test/16.jpg)  

  添加后：  
  ![](test/17.jpg)  

  ②	master*修改内容  
  ```
  <configuration>
<property>
    <name>dfs.namenode.http-address</name>
    <value>master1:50070</value>
</property>
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>master1:50090</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/opt/data/hadoop/nameNode</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///opt/data/hadoop/dataNode</value>
</property>
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.webhdfs.enable</name>
    <value>true</value>
</property>
</configuration>
    ```  
    添加前：  
    ![](test/18.jpg)  

    添加后：  
    ![](test/19.jpg)  

9.	修改配置文件mapred-site.xml  
　　在目录 /opt/packages/hadoop-2.6.3/etc/hadoop/下
命令cp mapred-site.xml-template mapred-site.xml
修改配置文件mapred-site.xml，添加配置，添加内容为：
    ```  
    <configuration>
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>master1:10020</value>
</property>
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>master1:19888</value>
</property>
</configuration>
    ```
    添加前：  
    ![](test/20.jpg)  

    添加后：  
    ![](test/21.jpg)  

10.	修改配置文件yarn-site.xml  
　　在目录 /opt/packages/hadoop-2.6.3/etc/hadoop/下
修改配置文件yarn-site.xml，添加配置，添加内容为：  
    ```  
    <configuration>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandle</value>
</property>
<property>
    <name>yarn.resourcemanager.address</name>
    <value>master1:8032</value>
</property>
<property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>master1:8030</value>
</property>
<property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>master1:8031</value>
</property>
<property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>master1:8033</value>
</property>
<property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>master1:8088</value>
</property>
</configuration>
    ```  
    修改前：  
    ![](test/22.jpg)  

    修改前：  
    ![](test/23.jpg)  

11.	Master\*修改配置文件slavers  
    master*节点修改配置文件slaves，添加配置，添加内容为各节点名称：
    ```
    slave1  
    slave2  
    ```  
12.	格式化一个新的分布式文件系统（只在master\*运行命令）  
    ```  
    $HADOOP_HOME/bin/hdfs  namenode  –format
    ```
　　退出状态为0时，表示格式化成功。  
    ![](test/24.jpg)  

13.	启动集群（只在master*运行命令)  
    ```  
    $HADOOP_HOME/sbin/start-all.sh  
    ```  
    ![](test/25.jpg)  

13.	启动集群（只在master*运行命令）  
    ```  
    $HADOOP_HOME/sbin/start-all.sh  
    ```  
    ![](test/26.jpg)  

14.	验证集群  
    ```
    jps  
    ```  
    ①	master*要有ResourceManager、SecondaryNameNode、NameNode进程  
    ![](test/27.jpg)  

    ②	slave*要有DataNode、NodeManager进程  
    ![](test/28.jpg)  

15.	查看集群状态  
    ```
    /$HADOOP_HOME/bin/hdfs dfsadmin –report  
    ```  
    ![](test/29.jpg)  

## 五、安装Hive（只在master\*上安装）  
1.	修改本地软件源  
　　将软件源修改为本地206服务器。  
    ```  
    sudo mv /etc/apt/sources.list /etc/apt/sources.list_163  
    sudo mv /etc/apt/sources.list_206 /etc/apt/sources.list  
    sudo apt-get update  
    ```  
2.	安装mysq  (分配数据库用户名，密码为root)
    ```  
    sudo apt-get install mysql-server mysql-client libmysqlclient-dev  
    ```  
3.	下载hive  
　　下载apache-hive-1.2.1-bin.tar.gz至/opt/sofaware；  
4.	解压hive  
　　解压apache-hive-1.2.1-bin.tar.gz至/opt/packages  
    ```  
    tar zxvf  /opt/sofawares/apache-hive-1.2.1-bin.tar.gz  –C  /opt/packages  
    ```  
  ![](test/30.jpg)  

5.	创建hive软链接  
　　在/opt/modules下创建hive软链接至/opt/packages/apache-hive-1.2.1-bin  
    ```  
    ln  –s /opt/packages/apache-hive-1.2.1-bin /  /opt/modules/hive  
    ```  
6.	配置hive环境变量  
    ```  
    sudo vim /etc/profile.d/cluster.sh  
    ```  
    在cluster.sh中增加下图内容：  
    ```  
    export HIVE_HOME=/opt/modules/hive  
    export PATH=$PATH:$HIVE_HOME/bin  
    ```  
    ![](test/31.jpg)  

7.	修改hive配置文件（$HIVE_HOME/conf）位置在 /opt/modules/hive/conf下  
　　把几个带.template后缀的模板文件，复制一份变成不带.template的配置文件，  
注意hive-default.xml.template这个要复制二份，一个是hive-default.xml，另一个是hive-site.xml，  
其中hive-site.xml为用户自定义配置，hive-default.xml为全局配置，  
hive启动时，-site.xml自定义配置会覆盖-default.xml全局配置的相同配置项。  
    ```  
    cp hive-default.xml.template hive-default.xml  
    cp hive-default.xml.template hive-site.xml  
    cp hive-exec-log4j.properties.template hive-exec-log4j.properties  
    cp hive-log4j.properties.template hive-log4j.properties  
    cp beeline-log4j.properties.template beeline-log4j.properties  
    cp hive-env.sh.template hive-env.sh  
    ```  
    ![](test/32.jpg)  

8.	修改hive-site.xml  
    ```
    <property>
<name>javax.jdo.option.ConnectionURL </name>
<value>jdbc:mysql://localhost:3306/hive </value>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName </name>
<value>com.mysql.jdbc.Driver </value>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword </name>
<value>hive </value>
</property>
<property>
<name>hive.hwi.listen.port </name>
<value>9999 </value>
<description>This is the port the Hive Web Interface will listen on </descript ion>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>
<description>Username to use against metastore database</description>
</property>
<property>
<name>hive.exec.local.scratchdir</name>
<value>/opt/data/hive/iotmp</value>
<description>Local scratch space for Hive jobs</description>
</property>
<property>
<name>hive.downloaded.resources.dir</name>
<value>/opt/data/hive/iotmp</value>
<description>Temporary local directory for added resources in the remote file system.</description>
</property>
<property>
<name>hive.querylog.location</name>
<value>/opt/data/hive/iotmp</value>
<description>Location of Hive run time structured log file</description>
</property>
<property>
<name>hive.metastore.uris</name>
<value>thrift://master:9083</value>            
</property>
    ```
9.	下载依赖包  
　　下载mysql-connector-java-5.1.38-bin.jar至$HIVE_HOME/lib目录下。  
10.	更新依赖包  
　　拷贝/opt/modules/hive/lib/jline-2.12.jar至/opt/modules/hadoop/share/hadoop/yarn/lib，  
并且把/opt/modules/hadoop/share/hadoop/yarn/lib下的jline-0.9.94.jar   
重命名为jline-0.9.94.jar_bak  

    ![](test/33.jpg)  

11.	启动mysql  
    ```  
    /etc/init.d/mysql start  
    ```  
12.	创建用户为hive的数据库  
    ```  
    create user 'hive'@'%' identified by 'hive';  
    grant all on *.* to hive@'%'  identified by 'hive';  
    flush privileges;  
    ```  
13.	修改mysql配置文件  
    ```  
    vim /etc/mysql/my.cnf  
    ```  
    ![](test/34.jpg)  
14.	启动hive  
    ```  
    /opt/modules/hive/bin/hive --service metastore &  
    ```
　　另起一个端口，输入hive，便可查询hive是否启动。   
## 六、	安装scala（master\*和slave\*都安装）  
1.	下载scala  
　　下载scala-2.10.4.tgz至/opt/sofawares；  
2.	解压scala  
　　解压scala-2.10.4.tgz至/opt/packages；  
    ```  
    tar zxvf  /opt/sofawares/scala-2.10.4.tgz  –C  /opt/packages  
    ```  
3.	创建scala软链接  
　　在/opt/modules下创建scala软链接至/opt/packages/scala-2.10.4  
    ```  
    ln –s /opt/packages/scala-2.10.4  /opt/modules/scala  
    ```  
4.	配置scala环境变量  
    ```  
    sudo vim /etc/profile.d/cluster.sh  
    ```  
　　在cluster.sh中添加下列内容  
    ```  
    export SCALA_HOME=/opt/modules/scala  
    export PATH=$PATH:$SCALA_HOME/bin  
    ```  
    ![](test/35.jpg)  

## 七、安装spark（master\*和slave\*都安装）  
1.	下载spark  
　　下载spark-1.6.1-bin-hadoop2.6.tgz至/opt/sofawares  
2.	解压spark  
　　解压spark-1.6.1-bin-hadoop2.6.tgz至/opt/packages;  
    ```  
    tar zxvf  /opt/sofawares/scala-2.10.4.tgz  –C  /opt/packages  
    ```  
3.	创建spark软链接  
    在/opt/modules下创建spark软链接至/opt/packages/spark-1.6.1-bin-hadoop2.6  
    ```  
    ln –s /opt/ spark-1.6.1-bin-hadoop2.6  /opt/modules/spark  
    ```  
4.	配置spark环境变量  
    ```  
    sudo vim /etc/profile.d/cluster.sh  
    ```  
    在cluster.sh中添加下列内容  
    ```  
    export SPARK_HOME=/opt/modules/spark  
    export PATH=$PATH:$SPARK_HOME/bin  
    ```  
    ![](test/36.jpg)  

5.	修改配置文件spark-env.sh  
　　修改配置文件spark-env.sh，在/opt/modules/spark/con下  
    ```  
    cp spark-env.sh.template spark-env.sh  
    vim spark-env.sh  
    ```  
    在最后一行增加下面内容：  
    ```  
    export JAVA_HOME=/opt/modules/jdk  
    export SCALA_HOME=/opt/modules/scala  
    export SPARK_MASTER_IP=master1  
    export SPARK_WORKER_MEMORY=1g  
    export HADOOP_CONF_DIR=/opt/modules/hadoop/etc/hadoop  
    ```  
    ![](test/37.jpg)  
6.	修改配置文件slaves（主节点master上配置）  
　　修改配置文件slaves，在/opt/modules/spark/con下  
    ```  
    cp slaves.template  slaves  
    vim slaves  
    ```  
    删除第19行的localhost，增加内容slave1  
    ![](test/38.jpg)  

7.	启动spark（只在master\*启动）  
    ```  
    $SPARK_HOME/sbin/start-all.sh  
    ```
    master1使用jps查看进程要有Master进程  
    ![](test/39.jpg)  

    Slave1使用jps查看要有worker进程  
    ![](test/40.jpg)  
8.	spark-sql与hive集成（主节点master上配置）  
    ①	在spark配置路径下(/opt/modules/spark/conf)，创建hive-site.xml，添加如下内容  
    ```  
    <configuration>
      <property>
        <name>hive.metastore.uri</name>
        <value>thrift://master1:9083</value>
      </property>
      <property>
        <name>hive.servier2.thirft.bind.host</name>
        <value>master1</value>
      </property>
    </configuration>  
    ```  
    ②	启动spark-sql （只在master\*启动)  
    ```  
    spark-sql $SPARK_HOME/bin/spark-sql --master spark://master1:7077  
    ```  
9.	启动thriftservier（只在master\*启动）  
    ```  
    $SPARK_HOME/sbin/start-thriftserver.sh --master spark://master1:7077  
    ```  
#� �F�i�l�e�
�
�
